{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "# Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Zwoi9UVegQJ",
        "outputId": "00fc36a0-71df-4b3d-c7df-3f948711fd51"
      },
      "outputs": [],
      "source": [
        "# !pip install unsloth\n",
        "# # Also get the latest nightly Unsloth!\n",
        "# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "dODEL0PcNXTq",
        "outputId": "9543a1d8-3332-416f-c659-e6cf6350339e"
      },
      "outputs": [],
      "source": [
        "from unsloth import tokenizer_utils\n",
        "def do_nothing(*args, **kwargs):\n",
        "    pass\n",
        "tokenizer_utils.fix_untrained_tokens = do_nothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBqQIc8ONXTq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "print(f\"Major: {major_version}, Minor: {minor_version}\")\n",
        "from datasets import load_dataset\n",
        "import datasets\n",
        "from trl import SFTTrainer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from typing import Tuple\n",
        "import warnings\n",
        "from typing import Any, Dict, List, Union\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "max_seq_length = 2048 \n",
        "dtype = None \n",
        "\n",
        "models_list = [\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    \"unsloth/Qwen2-7B-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-9b-it-bnb-4bit\",\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
        "]\n",
        "\n",
        "load_in_4bit = True\n",
        "\n",
        "model_name = models_list[0]\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i037LVONXTs"
      },
      "outputs": [],
      "source": [
        "yes_token_id = tokenizer.encode(\"Sim\", add_special_tokens=False)[0]\n",
        "no_token_id = tokenizer.encode(\"Não\", add_special_tokens=False)[0]\n",
        "# keep only the yes and no tokens from lm_head\n",
        "par = torch.nn.Parameter(torch.vstack([model.lm_head.weight[no_token_id, :], model.lm_head.weight[yes_token_id, :]]))\n",
        "print(par.shape)\n",
        "print(model.lm_head.weight.shape)\n",
        "model.lm_head.weight = par"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtDGkoflNXTs"
      },
      "outputs": [],
      "source": [
        "from peft import LoftQConfig\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\n",
        "        \"lm_head\", # can easily be trained because it has only 2 tokens\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,  # We support rank stabilized LoRA\n",
        "    # init_lora_weights = 'loftq',\n",
        "    # loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1), # And LoftQ\n",
        ")\n",
        "print(\"trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuG1L-qMNXTs",
        "outputId": "a240ed50-81cb-44be-e242-c36e97afdda4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "\n",
        "with open(\"base_de_dados/dataset_llm.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = pd.read_json(f, lines=True)\n",
        "\n",
        "def extrair_campos(messages):\n",
        "    user_text = None\n",
        "    assistant_text = None\n",
        "    for mensagem in messages:\n",
        "        if mensagem['role'] == 'user':\n",
        "            user_text = mensagem['content']\n",
        "        elif mensagem['role'] == 'assistant':\n",
        "            assistant_text = mensagem['content']\n",
        "    return user_text, assistant_text\n",
        "\n",
        "data[['text', 'assistant']] = data['messages'].apply(\n",
        "    lambda msgs: pd.Series(extrair_campos(msgs))\n",
        ")\n",
        "\n",
        "def converter_label(resposta):\n",
        "    resposta = resposta.strip().lower()\n",
        "    if resposta == 'similar':\n",
        "        return 1\n",
        "    elif resposta == 'diferente':\n",
        "        return 0\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "data['label'] = data['assistant'].apply(converter_label)\n",
        "\n",
        "final_df = data[['text', 'label']]\n",
        "\n",
        "train_size = 29368\n",
        "val_size = 7342\n",
        "\n",
        "data_sample = final_df.sample(n=train_size + val_size, random_state=24)\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "    data_sample, test_size=val_size/len(data_sample), random_state=24\n",
        ")\n",
        "\n",
        "print(\"len(train_df):\", len(train_df))\n",
        "print(\"len(val_df):\", len(val_df))\n",
        "print(\"Test size ratio:\", val_size/len(data_sample))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pe29IhZM5qNE",
        "outputId": "76f6dc17-503b-4100-9299-109e66355756"
      },
      "outputs": [],
      "source": [
        "print(val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "XEm_UiXzNXTt",
        "outputId": "0c6f3f60-e960-4c1c-c128-83236dbe12d2"
      },
      "outputs": [],
      "source": [
        "token_counts = [len(tokenizer.encode(x)) for x in train_df.text]\n",
        "# plot the token counts\n",
        "a = plt.hist(token_counts, bins=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.Dataset.from_pandas(train_df,preserve_index=False)\n",
        "train_dataset\n",
        "\n",
        "prompt = \"\"\"Aqui estão duas marcas:  \n",
        "{}  \n",
        "\n",
        "Essas duas marcas são similares? Responda apenas com \"Sim\" ou \"Não\".  \n",
        "\n",
        "SOLUÇÃO  \n",
        "A resposta correta é: \"{}\"\"\"\n",
        "\n",
        "positivelabel = \"Sim\"\n",
        "negativelabel = \"Não\"\n",
        "\n",
        "\n",
        "def formatting_prompts_func(dataset_):\n",
        "    # this is to fix an issue with a certain transformers version, you might not need this\n",
        "    if isinstance(dataset_['text'], str):\n",
        "        if model_name.lower().__contains__(\"qwen\"):\n",
        "            return [\"\"]*100\n",
        "        elif model_name.lower().__contains__(\"llama\"):\n",
        "            return \" \"\n",
        "        else:\n",
        "            return \" \"\n",
        "\n",
        "    texts = []\n",
        "    for i in range(len(dataset_['text'])):\n",
        "        t = dataset_['text'][i]\n",
        "        label = positivelabel if dataset_['label'][i] == 1 else negativelabel\n",
        "        text = prompt.format(t, label)\n",
        "\n",
        "        texts.append(text)\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MCM3kZMNXTt"
      },
      "outputs": [],
      "source": [
        "# this custom collator is needed to change the sequence labels from yes_token_id and no_token_id to 1 and 0.\n",
        "# It also trains only on the last token of the sequence.\n",
        "class DataCollatorForLastTokenLM(DataCollatorForLanguageModeling):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *args,\n",
        "        mlm: bool = False,\n",
        "        ignore_index: int = -100,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(*args, mlm=mlm, **kwargs)\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
        "        batch = super().torch_call(examples)\n",
        "\n",
        "        for i in range(len(examples)):\n",
        "            # Find the last non-padding token\n",
        "            last_token_idx = (batch[\"labels\"][i] != self.ignore_index).nonzero()[-1].item()\n",
        "            # Set all labels to ignore_index except for the last token\n",
        "            batch[\"labels\"][i, :last_token_idx] = self.ignore_index\n",
        "            # The old labels for the Yes and No tokens need to be mapped to 1 and 0\n",
        "            batch[\"labels\"][i, last_token_idx] = 1 if batch[\"labels\"][i, last_token_idx] == yes_token_id else 0\n",
        "\n",
        "\n",
        "        return batch\n",
        "collator = DataCollatorForLastTokenLM(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "4846f3f529974ee1957dc0817666832e",
            "afc783ed4da74be995b5f79e15b8a93b",
            "769663bc44c44cca8aa52d449080be53",
            "4b412dda02ac46f18dad183ece79cea0",
            "a7eb84d15c4c4b1bba59cd7815dff1f8",
            "504c0a34a6e84a5695d1c643f85f4cc0",
            "7d7b43203d534377ab37baafed993b9d",
            "93b97e3111234d84816b89feec82ca27",
            "a39f71bf09e649a3a3ba8850c473940b",
            "1fc36773b60b4f64b33a42be9446d26d",
            "431f9590d2334ddbb8c2dfbe6e1b2469"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "a23f0d1f-ad7d-4871-9d46-2d8059200ce6"
      },
      "outputs": [],
      "source": [
        "#APLICAR EALYR STOPPING\n",
        "from transformers import TrainerCallback, TrainingArguments\n",
        "import torch\n",
        "\n",
        "class EarlyStoppingCallback(TrainerCallback):\n",
        "    def __init__(self, threshold=0.0):\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        \"\"\"Verifica se a loss atingiu o limite e para o treinamento.\"\"\"\n",
        "        if state.log_history and \"loss\" in state.log_history[-1]:  # Checa se há logs de loss\n",
        "            current_loss = state.log_history[-1][\"loss\"]\n",
        "            print(f\"Passo {state.global_step}: Loss = {current_loss:.6f}\")\n",
        "\n",
        "            if current_loss <= self.threshold:\n",
        "                print(f\"🎯 Early Stopping ativado! Loss atingiu {current_loss:.6f}. Encerrando o treinamento...\")\n",
        "                control.should_training_stop = True  # Para o treinamento\n",
        "\n",
        "# Adicionando o callback no SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,  # not needed because group_by_length is True\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=32,\n",
        "        gradient_accumulation_steps=1,\n",
        "        warmup_steps=10,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        num_train_epochs=1,\n",
        "        max_steps=100,\n",
        "        report_to=\"none\",\n",
        "        group_by_length=True,\n",
        "    ),\n",
        "    formatting_func=formatting_prompts_func,\n",
        "    data_collator=collator,\n",
        "    callbacks=[EarlyStoppingCallback(threshold=0.0)],  # Adiciona o Early Stopping\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "f7b571e9-0e9a-4d86-a33b-d04c34ce078d"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extrai os logs do treinamento\n",
        "logs = trainer.state.log_history\n",
        "\n",
        "# Filtra os logs que contenham a métrica de perda (loss)\n",
        "steps = [log[\"step\"] for log in logs if \"loss\" in log]\n",
        "losses = [log[\"loss\"] for log in logs if \"loss\" in log]\n",
        "\n",
        "# Cria o gráfico\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(steps, losses, marker=\"o\", label=\"Training Loss\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Gráfico da Loss durante o Treinamento\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.savefig(f\"Llama-3-2-1B_loss.jpeg\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUerb4UsNXTv",
        "outputId": "9046934a-e0e7-4d25-ef7a-ae14e4fd16b4"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_mWyYyBNXTv"
      },
      "outputs": [],
      "source": [
        "saved_name = f\"lora_model_{model_name.replace('/','_')}\"\n",
        "model.save_pretrained(saved_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBivvKHhNXTv",
        "outputId": "0d212964-39f3-49fe-bfe2-61c7bbcebe24"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, auc\n",
        "import torch.nn.functional as nnf\n",
        "\n",
        "# Step 1: Tokenize the inputs and sort them by their tokenized length\n",
        "tokenized_inputs = []\n",
        "for i in range(len(val_df['text'])):\n",
        "    text = val_df['text'].iloc[i]\n",
        "    test_str = prompt.format(text, \"\")\n",
        "    tokenized_input = tokenizer(test_str, return_tensors=\"pt\", add_special_tokens=False)\n",
        "    tokenized_inputs.append((tokenized_input, test_str, val_df['label'].iloc[i]))\n",
        "\n",
        "# Sort by tokenized length\n",
        "tokenized_inputs.sort(key=lambda x: x[0]['input_ids'].shape[1])\n",
        "\n",
        "# Step 2: Group the inputs by their tokenized length\n",
        "grouped_inputs = defaultdict(list)\n",
        "for tokenized_input, test_str, label in tokenized_inputs:\n",
        "    length = tokenized_input['input_ids'].shape[1]\n",
        "    grouped_inputs[length].append((tokenized_input, test_str, label))\n",
        "\n",
        "# Step 3: Process each group in batches of 64\n",
        "batch_size = 64\n",
        "all_outputs = []\n",
        "all_strings = []\n",
        "all_labels = []\n",
        "all_probabilities = []\n",
        "\n",
        "from tqdm import tqdm\n",
        "for length, group in tqdm(grouped_inputs.items()):\n",
        "    for i in range(0, len(group), batch_size):\n",
        "        batch = group[i:i+batch_size]\n",
        "        batch_inputs = [item[0] for item in batch]\n",
        "        batch_strings = [item[1] for item in batch]\n",
        "        batch_labels = [item[2] for item in batch]\n",
        "\n",
        "        # Concatenate the batch inputs\n",
        "        input_ids = torch.cat([item['input_ids'] for item in batch_inputs], dim=0).to(\"cuda\")\n",
        "        attention_mask = torch.cat([item['attention_mask'] for item in batch_inputs], dim=0).to(\"cuda\")\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            # print(outputs.logits[:, -1].shape)\n",
        "\n",
        "        # Get logits for the first token prediction (assuming binary classification)\n",
        "        logits = outputs.logits[:, -1, :2]  # Only consider logits for 0 and 1\n",
        "\n",
        "        # Apply softmax\n",
        "        probabilities = F.softmax(logits, dim=-1)\n",
        "        test = F.softmax(logits, dim=1)[:,1]\n",
        "        # print(\"test\")\n",
        "        # print(test.shape)\n",
        "        # top_p, top_class = probabilities.topk(1, dim=-1)\n",
        "        value = [test.cpu().numpy() for t in test]\n",
        "        for x in value[0]:\n",
        "        #   print(x)\n",
        "          all_probabilities.append(x)\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = torch.argmax(probabilities, dim=-1)\n",
        "\n",
        "        all_outputs.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(batch_labels)\n",
        "        all_strings.extend(batch_strings)\n",
        "\n",
        "# print(\"all_probabilities\")\n",
        "# print(all_probabilities)\n",
        "\n",
        "all_labelst = torch.tensor(all_labels)\n",
        "all_predst = torch.tensor(all_outputs)\n",
        "\n",
        "# all_probabilities\n",
        "all_probabilitiest = torch.tensor(all_probabilities)\n",
        "# # ROC Curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(all_labelst.numpy(), all_probabilitiest.numpy())\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print(\"ROC AUC: \", roc_auc)\n",
        "\n",
        "# Step 4: Do the label assignment\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(all_labelst.numpy(), all_predst.numpy())\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(all_labelst.numpy(), all_predst.numpy())\n",
        "# Precision\n",
        "precision = precision_score(all_labelst.numpy(), all_predst.numpy())\n",
        "# Recall\n",
        "recall = recall_score(all_labelst.numpy(), all_predst.numpy())\n",
        "# F1-Score\n",
        "f1 = f1_score(all_labelst.numpy(), all_predst.numpy())\n",
        "\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1-Score: \", f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbKe-uT66ja9",
        "outputId": "b8e6a3d1-fee2-4da4-d2f9-4a0847ed0e22"
      },
      "outputs": [],
      "source": [
        "print(\"all_labels\")\n",
        "print(all_labels)\n",
        "\n",
        "print(\"all_outputs\")\n",
        "print(all_outputs)\n",
        "\n",
        "all_labelst = torch.tensor(all_labels)\n",
        "all_predst = torch.tensor(all_outputs)\n",
        "\n",
        "# Step 4: Do the label assignment\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(all_labelst.numpy(), all_predst.numpy())\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(all_labelst.numpy(), all_predst.numpy())\n",
        "# Precision\n",
        "precision = precision_score(all_labelst.numpy(), all_predst.numpy())\n",
        "# Recall\n",
        "recall = recall_score(all_labelst.numpy(), all_predst.numpy())\n",
        "# F1-Score\n",
        "f1 = f1_score(all_labelst.numpy(), all_predst.numpy())\n",
        "# ROC Curve and AUC\n",
        "# fpr, tpr, thresholds = roc_curve(all_labelst.numpy(), all_predst.numpy())\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# print(\"fpr\")\n",
        "# print(fpr)\n",
        "# print(\"tpr\")\n",
        "# print(tpr)\n",
        "# print(\"thresholds\")\n",
        "# print(thresholds)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1-Score: \", f1)\n",
        "print(\"ROC AUC: \", roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "id": "2IBHEPMrWLiJ",
        "outputId": "7f390cc0-522c-492b-b9f7-bdbbc7cb29dd"
      },
      "outputs": [],
      "source": [
        "# print(fpr)\n",
        "# print(tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "plt.plot(fpr, tpr, label='ROC Curve')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "model_metrics = {\n",
        "    \"confusion_matrix\": cm,\n",
        "    \"accuracy\": accuracy,\n",
        "    \"precision\": precision,\n",
        "    \"recall\": recall,\n",
        "    \"f1\": f1\n",
        "}\n",
        "\n",
        "model_results = {\n",
        "    \"metrics\": model_metrics,\n",
        "    \"roc\": {\n",
        "        \"fpr\": fpr,\n",
        "        \"tpr\": tpr\n",
        "    }\n",
        "}\n",
        "\n",
        "filename = \"model_results.pkl\"\n",
        "if os.path.exists(filename):\n",
        "    with open(filename, \"rb\") as file:\n",
        "        results_dict = pickle.load(file)\n",
        "else:\n",
        "    results_dict = {}\n",
        "\n",
        "results_dict[model_name] = model_results\n",
        "\n",
        "with open(filename, \"wb\") as file:\n",
        "    pickle.dump(results_dict, file)\n",
        "\n",
        "print(\"Métricas e curvas ROC salvas com sucesso para o modelo:\", model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "filename = \"model_results.pkl\"\n",
        "\n",
        "# Carregar o arquivo pickle\n",
        "with open(filename, \"rb\") as file:\n",
        "    results_dict = pickle.load(file)\n",
        "\n",
        "# Exibir todas as métricas salvas\n",
        "for model_result, data in results_dict.items():\n",
        "    print(f\"\\nModelo: {model_result}\")\n",
        "    print(\"Métricas:\")\n",
        "    for metric, value in data[\"metrics\"].items():\n",
        "        print(f\"  - {metric}: {value}\")\n",
        "\n",
        "    # print(\"\\nCurva ROC:\")\n",
        "    # print(f\"  - False Positive Rate (FPR): {data['roc']['fpr']}\")\n",
        "    # print(f\"  - True Positive Rate (TPR): {data['roc']['tpr']}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5081962,
          "sourceId": 8512897,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30733,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1fc36773b60b4f64b33a42be9446d26d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "431f9590d2334ddbb8c2dfbe6e1b2469": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4846f3f529974ee1957dc0817666832e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_afc783ed4da74be995b5f79e15b8a93b",
              "IPY_MODEL_769663bc44c44cca8aa52d449080be53",
              "IPY_MODEL_4b412dda02ac46f18dad183ece79cea0"
            ],
            "layout": "IPY_MODEL_a7eb84d15c4c4b1bba59cd7815dff1f8"
          }
        },
        "4b412dda02ac46f18dad183ece79cea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fc36773b60b4f64b33a42be9446d26d",
            "placeholder": "​",
            "style": "IPY_MODEL_431f9590d2334ddbb8c2dfbe6e1b2469",
            "value": " 800/800 [00:03&lt;00:00, 294.87 examples/s]"
          }
        },
        "504c0a34a6e84a5695d1c643f85f4cc0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "769663bc44c44cca8aa52d449080be53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93b97e3111234d84816b89feec82ca27",
            "max": 800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a39f71bf09e649a3a3ba8850c473940b",
            "value": 800
          }
        },
        "7d7b43203d534377ab37baafed993b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93b97e3111234d84816b89feec82ca27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a39f71bf09e649a3a3ba8850c473940b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7eb84d15c4c4b1bba59cd7815dff1f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afc783ed4da74be995b5f79e15b8a93b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_504c0a34a6e84a5695d1c643f85f4cc0",
            "placeholder": "​",
            "style": "IPY_MODEL_7d7b43203d534377ab37baafed993b9d",
            "value": "Map (num_proc=2): 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
